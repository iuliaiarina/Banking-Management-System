{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iuliaiarina/Banking-Management-System/blob/main/Lab_4_Natural_Language_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91wJWNwvk5kg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import sys\n",
        "import traceback\n",
        "import re\n",
        "print(f\"Python {sys.version}\")\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "We need to make this determinitsic so we can keep a track of changes we do to the model\n",
        "If we are using the same initialisation all the time, then changes\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(317)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(317)\n",
        "\n",
        "try:\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('wordnet')\n",
        "  from nltk.corpus import stopwords\n",
        "  from nltk import WordNetLemmatizer\n",
        "except:\n",
        "  print(\"No nltk\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # DEVICE = 'cpu'\n",
        "    print(f\"PyTorch {torch.__version__}\")\n",
        "    print(f\"DEVICE={DEVICE}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"\\tGPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"\\t\\tcapability: {torch.cuda.get_device_capability('cuda')[0]}\")\n",
        "        print(f\"\\tCUDA version: {torch.version.cuda}\")\n",
        "        print(\"\\tcuDNN available: \", torch.backends.cudnn.is_available())\n",
        "\n",
        "        if torch.backends.cudnn.is_available():\n",
        "            print(\"\\t\\tcuDNN version: \", torch.backends.cudnn.version())\n",
        "\n",
        "        # Print the number of GPUs available\n",
        "        print(f\"\\tNumber of GPUs available: {torch.cuda.device_count()}\")\n",
        "        torch.manual_seed(317) # Moved this line here\n",
        "\n",
        "    from torch import nn\n",
        "    from torch.nn import functional as F\n",
        "    from torch.optim import AdamW\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "except:\n",
        "    print(\"No PyTorch\")\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "\n",
        "    print(f\"TensorFlow {tf.__version__}\")\n",
        "    print(f\"Build with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "    print(f\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "except:\n",
        "    print(\"No TensorFlow\")\n",
        "\n",
        "try:\n",
        "  from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "  from tqdm import tqdm\n",
        "  from datasets import load_dataset\n",
        "\n",
        "  #print(f\"Tokenizer version {BertTokenizer.__version__}\")\n",
        "except:\n",
        "  print(\"No Transformers\")\n",
        "\n",
        "try:\n",
        "  import sklearn\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  print(f\"scikit-learn {sklearn.__version__}\")\n",
        "except:\n",
        "  print(\"No scikit-learn\")\n",
        "\n",
        "# DEVICE=\"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Inference (NLI)\n",
        "\n",
        "Transformer Resources: [Jurafsky](https://web.stanford.edu/~jurafsky/slp3/) book - Chapter 8 and 10\n",
        "\n",
        "[BERT](https://arxiv.org/pdf/1810.04805) paper.\n",
        "\n"
      ],
      "metadata": {
        "id": "1xUcaqRcGmZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Exercise 1] Train BERT model on SNLI\n",
        "\n",
        "We train a BERT-based model for Natural Language Inference (NLI) on the SNLI dataset, where each example consists of a `premise` and a `hypothesis` labeled as entailment, neutral, or contradiction.\n",
        "\n",
        "For example, given the pair:\n",
        "1. “A man is playing a guitar”  \n",
        "2. “A person is performing music”\n",
        "\n",
        "the correct label is entailment.\n",
        "\n",
        "The training process involves `tokenizing` sentence pairs with the BERT tokenizer, creating a PyTorch dataset and dataloader, and fine-tuning `BertForSequenceClassification` with three output labels. We evaluate in-domain performance on the SNLI validation set for each epoch, choosing the best model.\n",
        "\n",
        "Then we test the model on the test dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "IMwRBMMkPSWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 1.1] Load and visualize the data\n",
        "Required Actions:\n",
        "\n",
        "* Load: Load the [SNLI](https://huggingface.co/datasets/stanfordnlp/snli) dataset using the `datasets` library.\n",
        "\n",
        "* Clean: Define a cleaning procedure that converts the label column to an integer type and filters out all rows where the label is not 0, 1, or 2.\n",
        "\n",
        "* Sample: Subset the cleaned data splits to manageable sizes: 3,000 samples for training, and 1,000 samples each for validation and testing.\n"
      ],
      "metadata": {
        "id": "xU4ONVvFhwrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snli = load_dataset(\"snli\")\n",
        "print(snli)\n",
        "\n",
        "def clean_df_by_label(df):\n",
        "    # ==================\n",
        "    # YOUR CODE HERE\n",
        "    # ==================\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# snli_train_df = clean_df_by_label(snli[\"train\"].to_pandas())\n",
        "# ...\n",
        "# ==================\n",
        "\n",
        "print(snli_train_df.head(10))"
      ],
      "metadata": {
        "id": "cW6piOynNZUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 1.2] Create the Dataset class\n",
        "\n",
        "Required Actions (Implementation of `NLIDataset`):\n",
        "\n",
        "* Initialization: Accept a Pandas DataFrame, a pre-trained tokenizer, and a maximum sequence length (max_length).\n",
        "\n",
        "* Label Conversion: Extract the label column from the DataFrame and convert it directly into a PyTorch Long Tensor.\n",
        "\n",
        "* Text Encoding: Tokenize the pairs of premise and hypothesis columns:\n",
        "\n",
        "  1. Apply padding and truncation based on the provided max_length.\n",
        "\n",
        "  2. Ensure the output tensors are in PyTorch format (return_tensors='pt').\n",
        "\n",
        "  3. Exclude token type IDs (return_token_type_ids=False).\n",
        "\n",
        "* Interface Implementation: Implement the mandatory __len__ method to return the dataset size, and __getitem__ method to return a dictionary containing `input_ids`, `attention_mask`, and `labels`."
      ],
      "metadata": {
        "id": "uzjsXgeJhiLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLIDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # ==================\n",
        "\n",
        "    def __len__(self):\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # ==================\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # =================="
      ],
      "metadata": {
        "id": "ZJmYKggkg_al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 1.3] Create the model class.\n",
        "\n",
        "Required Actions (Implementation of NLIModel):\n",
        "\n",
        "* Initialization: Load `BertForSequenceClassification` with configurable model_name and num_labels=3.\n",
        "\n",
        "* Forward Pass: Define forward pass.\n"
      ],
      "metadata": {
        "id": "RKKYH3oukO_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLIModel(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=3):\n",
        "        super(NLIModel, self).__init__()\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # ==================\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # ==================\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "SfmfrRZ-kavG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 1.4] Construct the training loop and train your model\n",
        "\n",
        "Required Actions (Implementation of train_classifier):\n",
        "\n",
        "* Setup: Initialize NLIModel (using bert-base-uncased), AdamW optimizer, and CrossEntropyLoss.\n",
        "\n",
        "* Training Loop: Iterate over epochs, setting the model to train(), performing the forward pass, calculating loss, backpropagating, and stepping the optimizer for each batch.\n",
        "\n",
        "* Validation Loop: Iterate over validation data using torch.no_grad(), setting the model to eval(), and calculating validation loss and accuracy.\n",
        "\n",
        "* Checkpointing: Implement model saving (torch.save(model.state_dict(), \"model.pt\")) based on achieving the minimum validation loss.\n",
        "\n",
        "* Logging: Calculate and print epoch-level average training loss/accuracy and validation loss/accuracy.\n",
        "\n",
        "* Output: Return the final trained model object."
      ],
      "metadata": {
        "id": "LX73SohMkuhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(train_loader, val_loader, epochs=7, lr=1e-5):\n",
        "\n",
        "    # ==================\n",
        "    # YOUR CODE HERE\n",
        "    # ==================\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # ==================\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "\n",
        "            # ==================\n",
        "            # YOUR CODE HERE\n",
        "            # ==================\n",
        "\n",
        "        # ==================\n",
        "        # YOUR CODE HERE\n",
        "        # ==================\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader):\n",
        "\n",
        "              # ==================\n",
        "              # YOUR CODE HERE\n",
        "              # ==================\n",
        "\n",
        "              if min_val_loss > total_val_loss/len(val_loader):\n",
        "                  min_val_loss = total_val_loss/len(val_loader)\n",
        "                  torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: \\nTrain Loss = {total_train_loss/len(train_loader):.4f} Train Acc = {total_train_correct/total_train_items:.4f}\\nVal Loss = {total_val_loss/len(val_loader):.4f} Val Acc = {total_val_correct/total_val_items:.4f}\")\n",
        "\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "DnoE6ROMkyyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 1.5] Construct the test loop and inference function\n",
        "\n",
        "Required Actions (Implementation of test and inference):\n",
        "\n",
        "* Test Function (test):\n",
        "\n",
        "* Enter evaluation mode (model.eval()) and disable gradient tracking (torch.no_grad()).\n",
        "\n",
        "* Iterate over the test_loader to calculate cumulative loss and correct predictions.\n",
        "\n",
        "* Compute and print the final average test loss and classification accuracy.\n",
        "\n",
        "Inference Function (inference):\n",
        "\n",
        "* Tokenize the input premise and hypothesis using the provided tokenizer.\n",
        "\n",
        "* Perform a forward pass on the model using the tokenized inputs (without labels).\n",
        "\n",
        "* Determine the final class prediction by taking the argmax of the output logits.\n"
      ],
      "metadata": {
        "id": "jo5H1ZXIH8_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    # ==================\n",
        "    # YOUR CODE HERE\n",
        "    # ==================\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in test_loader:\n",
        "          # ==================\n",
        "          # YOUR CODE HERE\n",
        "          # ==================\n",
        "\n",
        "\n",
        "    print(f\"Test Loss = {total_loss/len(test_loader):.4f} Accuracy: {total_correct/total_items:.4f}\")\n",
        "\n",
        "\n",
        "def inference(model, tokenizer, premise, hypothesis, max_length):\n",
        "    # ==================\n",
        "    # YOUR CODE HERE\n",
        "    # ==================\n",
        "\n",
        "    print(f\"Premise: {premise}\")\n",
        "    print(f\"Hypothesis: {hypothesis}\")\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    return predicted_class\n",
        "\n"
      ],
      "metadata": {
        "id": "cxDVDlAzfiz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 1.6] Put it all together\n",
        "\n",
        "Required Actions (Implementation of test and inference):\n",
        "\n",
        "* Initialize Tokenizer\n",
        "* Initialize the datasets (train, val, test)\n",
        "* Initialize the dataloaders\n",
        "* Train the model\n",
        "* Load saved model and run test loop\n",
        "* Run the `inference` method with an example\n"
      ],
      "metadata": {
        "id": "op_9TQGtKKRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# ==================\n",
        "print(\"Tokenizer initialized.\")\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# ==================\n",
        "print(\"Datasets initialized.\")\n",
        "\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# ==================\n",
        "print(\"Dataloaders initialized.\")\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# ==================\n",
        "print(\"Training Completed.\")\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# ==================\n",
        "print(\"Testing Completed.\")\n",
        "\n",
        "premise = \"The sun is shining on the clear blue sky.\"\n",
        "hypothesis = \"It is pouring outside.\"\n",
        "inference(model, tokenizer, premise, hypothesis, max_length)\n"
      ],
      "metadata": {
        "id": "fsNovtodlXlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 2] Out-of-distribution testing.\n",
        "\n",
        "NLI models are often trained and evaluated on the same dataset distribution (e.g., SNLI), achieving high in-domain accuracy.\n",
        "\n",
        "However, when tested on out-of-distribution (OOD) datasets — such as MNLI, SICK, or ANLI — model performance typically drops sharply.\n",
        "This suggests that the model may have learned dataset-specific biases or lexical artifacts, rather than robust inference skills.\n",
        "\n",
        "Try to test your model on another NLI dataset (MNLI). What can you observe? How is the model performing in an Out-of-distribution setting?"
      ],
      "metadata": {
        "id": "ELTnwjQkfTKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnli = load_dataset(\"SetFit/mnli\", split='validation')\n",
        "\n",
        "def clean_df_by_label(df):\n",
        "    # ==================\n",
        "    # YOUR CODE HERE\n",
        "    # ==================\n",
        "\n",
        "    return df\n",
        "\n",
        "print(mnli)\n",
        "\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# ==================\n",
        "mnli_val_df = mnli_val_df.rename(columns={'text1': 'premise', 'text2': 'hypothesis'})\n",
        "print(mnli_val_df.head(10))"
      ],
      "metadata": {
        "id": "grtnrx2afS59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model trained on SNLI on the MNLI val split\n",
        "# ==================\n",
        "# YOUR CODE HERE\n",
        "# =================="
      ],
      "metadata": {
        "id": "sAHNi_K88dGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 3] OOD solution?\n",
        "\n",
        "Think for a solution to solve the OOD issue (can be a subject of your project too). You can find an example [here](https://arxiv.org/abs/2502.09567).\n",
        "\n",
        "You can also try to evaluate the accuracy of your model on different datasets, and then try to compose a 'general' dataset and train on that. What happens?\n",
        "\n",
        "Another idea is to calibrate the prediction models and see if the OOD performance improves."
      ],
      "metadata": {
        "id": "S2SYdgtmfhjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wcPNf3Gf5w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Exercise 4] Artifacts in NLI\n",
        "\n",
        "What is the complexity of certain NLI datasets (SNLI, MNLI, SICK, FEVER)? What artifacts do they hide?\n",
        "\n",
        "An artifact refers to an unintended pattern or bias in the data that allows a model to make correct predictions without performing true inference.\n",
        "\n",
        "For example, in the SNLI dataset, certain words or lexical cues in the hypothesis (like “no”, “nobody”) strongly correlate with specific labels (e.g., contradiction), enabling models to classify examples correctly by exploiting surface heuristics rather than understanding the semantic relationship between premise and hypothesis.\n",
        "\n",
        "You can experiment by predicting the label on SNLI just focusing on the hypotheses. What other such problems can you find for different datasets?\n",
        "\n",
        "You can read [this](https://studenttheses.uu.nl/bitstream/handle/20.500.12932/40692/BA_thesis_improved.pdf?sequence=1&isAllowed=y) article."
      ],
      "metadata": {
        "id": "Zq6q993K9-wO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIIFyY1y-XNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Extra Work] Training a BERT from scratch\n",
        "\n",
        "If you want to comprehend better how transformers (and BERT) work, you can try to do this lab from another subject (Machine Learning 2), where you construct a BERT layer by layer, from scratch.\n",
        "\n",
        "[link](https://colab.research.google.com/drive/1NWHzFkeSCyd42RAOAV-caBYfWGWZiUsz?usp=sharing)"
      ],
      "metadata": {
        "id": "yboM9yMQhJj7"
      }
    }
  ]
}